{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Molecular_translation.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i3fKK_cdyNVz",
        "outputId": "e53376cd-48de-42b5-e22c-66a95088c849"
      },
      "source": [
        "!pip install --upgrade --force-reinstall --no-deps kaggle"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Processing /root/.cache/pip/wheels/a1/6a/26/d30b7499ff85a4a4593377a87ecf55f7d08af42f0de9b60303/kaggle-1.5.12-cp37-none-any.whl\n",
            "Installing collected packages: kaggle\n",
            "  Found existing installation: kaggle 1.5.12\n",
            "    Uninstalling kaggle-1.5.12:\n",
            "      Successfully uninstalled kaggle-1.5.12\n",
            "Successfully installed kaggle-1.5.12\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "91WNJZAOqDwc",
        "outputId": "f7967c3c-32cf-4419-b42f-5b8ee9e20d60"
      },
      "source": [
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!ls ~/.kaggle\n",
        "!chmod 600 /root/.kaggle/kaggle.json"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "kaggle.json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H4H_l203pMiD"
      },
      "source": [
        "!kaggle competitions download -c bms-molecular-translation  kaggle"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u61FMhsa7oWh"
      },
      "source": [
        "import zipfile\n",
        "archive = zipfile.ZipFile('/content/drive/MyDrive/bms-molecular-translation.zip')\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bCOnVHcdAynr"
      },
      "source": [
        "archive.namelist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w7U7y1MJ_akf"
      },
      "source": [
        "for file in archive.namelist():\n",
        "    if file.startswith('test'):\n",
        "        archive.extract(file, 'kaggle')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nUZ5diRN4vps"
      },
      "source": [
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import time\n",
        "from PIL import Image\n",
        "import glob"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WzcfMVt0M5_A"
      },
      "source": [
        "TOKEN_LIST = [\"<PAD>\", \"InChI=1S/\",\"<START>\", \"<END>\", \"/c\", \"/h\", \"/m\", \"/t\", \"/b\", \"/s\", \"/i\"] +\\\n",
        "             ['Si', 'Br', 'Cl', 'F', 'I', 'N', 'O', 'P', 'S', 'C', 'H', 'B', ] +\\\n",
        "             [str(i) for i in range(167,-1,-1)] +\\\n",
        "             [\"+\", \"(\", \")\", \"-\", \",\", \"D\", \"T\"]"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3bEDIpSMGfyN",
        "outputId": "72b91cca-d6ab-42da-d362-17153f9ce3c6"
      },
      "source": [
        "len(TOKEN_LIST)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "198"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 141
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8az3og1CX_WX"
      },
      "source": [
        "tok_2_int = {c:i for i,c in enumerate(TOKEN_LIST)}\n",
        "int_2_tok = {v:k for k,v in tok_2_int.items()}"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ML1IocuzjvF1"
      },
      "source": [
        "def convert_to_tensor(label):\n",
        "  token = [tok_2_int[\"<START>\"]]\n",
        "  l = label.split('/')\n",
        "  token.append(tok_2_int[l[0]+'/'])\n",
        "  f = re.split('(\\d+)', l[1])\n",
        "  for c in f:\n",
        "    if c.isnumeric()==False:\n",
        "      st=0\n",
        "      for i in range(len(c)+1):\n",
        "        if c[st:i] in TOKEN_LIST:\n",
        "          token.append(tok_2_int[c[st:i]])\n",
        "          st=i\n",
        "    else:\n",
        "      token.append(tok_2_int[c])\n",
        "\n",
        "  for i in range(2,len(l)):\n",
        "    token.append(tok_2_int['/'+l[i][0]])\n",
        "    s = re.split(r'(\\W+)', l[i][1:])\n",
        "    for c in s:\n",
        "      if c.isnumeric()==False and len(c)>=2:\n",
        "        if c[0] == '-' or c[0] == '+' or c[0] == ',' or c[0] == ')'  or c[0]== '(':\n",
        "          for sp in c:\n",
        "            token.append(tok_2_int[sp])\n",
        "        else:\n",
        "          cc = re.split('(\\d+)', c)\n",
        "          for b in cc:\n",
        "            if b.isnumeric()==False and len(b)>=2:\n",
        "              st=0\n",
        "              for i in range(len(b)+1):\n",
        "                if b[st:i] in TOKEN_LIST:\n",
        "                  token.append(tok_2_int[b[st:i]])\n",
        "                  st=i\n",
        "            else:\n",
        "               if len(b)>0:\n",
        "                 token.append(tok_2_int[b])\n",
        "\n",
        "      else:\n",
        "        if len(c)>0:\n",
        "          token.append(tok_2_int[c])\n",
        "  token.append(tok_2_int[\"<END>\"])\n",
        "  return token"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T4QFmgxCV-68"
      },
      "source": [
        "import pandas as pd\n",
        "labels = pd.read_csv('/content/kaggle/train_labels.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XDiMlallXHqe"
      },
      "source": [
        "labels = labels.sort_values('image_id')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "21-PkfJVigjH",
        "outputId": "9bdcb621-dc59-4024-fd5b-ab04f8b37750"
      },
      "source": [
        "labels.columns"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['image_id', 'InChI'], dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hQDn-ojNilxt"
      },
      "source": [
        "names = labels.iloc[:,1]\n",
        "id = labels.iloc[:,0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D8Q2hTHMi6n9"
      },
      "source": [
        "import glob\n",
        "images = glob.glob('/content/kaggle/train/*/*/*/*.png')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VdWRwhEAWiso"
      },
      "source": [
        "images = sorted(images)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u7DDTdsILKuE",
        "outputId": "d1eb1903-3430-4880-88bb-30a3c0204257"
      },
      "source": [
        "print(images[1])\n",
        "print(id[1])\n",
        "print(names[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/kaggle/train/0/0/0/000019cc0cd2.png\n",
            "000019cc0cd2\n",
            "InChI=1S/C21H30O4/c1-12(22)25-14-6-8-20(2)13(10-14)11-17(23)19-15-4-5-18(24)21(15,3)9-7-16(19)20/h13-16,19H,4-11H2,1-3H3/t13-,14+,15+,16-,19-,20+,21+/m1/s1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jGqhfIPzkxqR"
      },
      "source": [
        "names = names[:10000]\n",
        "images = images[:10000]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v917wicKjfEg",
        "outputId": "cedf4378-bf05-47b8-8d28-269eeacbc83a"
      },
      "source": [
        "len(images)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bb_e4V-ToFX4",
        "outputId": "7c31e8b1-f19b-49db-988f-174d5b218284"
      },
      "source": [
        "len(names)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QfDQaJyeoUr6"
      },
      "source": [
        "name_vectors = []\n",
        "for i in range(len(names)):\n",
        "  name_vectors.append(convert_to_tensor(names[i]))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xlCET-F3p9u4",
        "outputId": "8c912e36-16b3-4bb8-a394-0ca1838127eb"
      },
      "source": [
        "max_len = max(len(t) for t in name_vectors)\n",
        "print(max_len)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "271\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4WKJPQp5qJQd"
      },
      "source": [
        "name_vectors = tf.keras.preprocessing.sequence.pad_sequences(name_vectors, padding='post')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mdR2Hqnvu7MZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf34a8e7-3336-46e2-d6e6-f5ce756bd684"
      },
      "source": [
        "image_model = tf.keras.applications.InceptionV3(include_top=False,\n",
        "                                                weights='imagenet')\n",
        "new_input = image_model.input\n",
        "hidden_layer = image_model.layers[-1].output\n",
        "\n",
        "image_features_extract_model = tf.keras.Model(new_input, hidden_layer)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_v3/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "87916544/87910968 [==============================] - 2s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fPLkaqtkxo9A"
      },
      "source": [
        "BATCH_SIZE = 8\n",
        "BUFFER_SIZE = 1000\n",
        "embedding_dim = 256\n",
        "units = 512\n",
        "vocab_size = 198\n",
        "num_steps = len(images) // BATCH_SIZE\n",
        "# Shape of the vector extracted from InceptionV3 is (64, 2048)\n",
        "# These two variables represent that vector shape\n",
        "features_shape = 2048\n",
        "attention_features_shape = 64"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "25lp0lN6urOf"
      },
      "source": [
        "def load_image(image_path, name_vectors):\n",
        "    img = tf.io.read_file(image_path)\n",
        "    img = tf.image.decode_jpeg(img, channels=3)\n",
        "    img = tf.image.resize(img, (299, 299))\n",
        "    img = tf.keras.applications.inception_v3.preprocess_input(img)\n",
        "    batch_features = image_features_extract_model(tf.expand_dims(img,0))\n",
        "    batch_features = tf.reshape(batch_features[0],\n",
        "                              [64,2048])\n",
        "    return batch_features, name_vectors"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2A-60A1-uyN5"
      },
      "source": [
        "image_dataset = tf.data.Dataset.from_tensor_slices((images, name_vectors))\n",
        "image_dataset = image_dataset.map(\n",
        "  load_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "dataset = image_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
        "dataset = dataset.prefetch(buffer_size=tf.data.AUTOTUNE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XuBjcxGa-FZH"
      },
      "source": [
        "class BahdanauAttention(tf.keras.Model):\n",
        "  def __init__(self, units):\n",
        "    super(BahdanauAttention, self).__init__()\n",
        "    self.W1 = tf.keras.layers.Dense(units)\n",
        "    self.W2 = tf.keras.layers.Dense(units)\n",
        "    self.V = tf.keras.layers.Dense(1)\n",
        "\n",
        "  def call(self, features, hidden):\n",
        "    # features(CNN_encoder output) shape == (batch_size, 64, embedding_dim)\n",
        "\n",
        "    # hidden shape == (batch_size, hidden_size)\n",
        "    # hidden_with_time_axis shape == (batch_size, 1, hidden_size)\n",
        "    hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
        "\n",
        "    # attention_hidden_layer shape == (batch_size, 64, units)\n",
        "    attention_hidden_layer = (tf.nn.tanh(self.W1(features) +\n",
        "                                         self.W2(hidden_with_time_axis)))\n",
        "\n",
        "    # score shape == (batch_size, 64, 1)\n",
        "    # This gives you an unnormalized score for each image feature.\n",
        "    score = self.V(attention_hidden_layer)\n",
        "\n",
        "    # attention_weights shape == (batch_size, 64, 1)\n",
        "    attention_weights = tf.nn.softmax(score, axis=1)\n",
        "\n",
        "    # context_vector shape after sum == (batch_size, hidden_size)\n",
        "    context_vector = attention_weights * features\n",
        "    context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "\n",
        "    return context_vector, attention_weights"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lYny8mCBC_nP"
      },
      "source": [
        "class CNN_Encoder(tf.keras.Model):\n",
        "    # Since you have already extracted the features and dumped it\n",
        "    # This encoder passes those features through a Fully connected layer\n",
        "    def __init__(self, embedding_dim):\n",
        "        super(CNN_Encoder, self).__init__()\n",
        "        # shape after fc == (batch_size, 64, embedding_dim)\n",
        "        self.fc = tf.keras.layers.Dense(embedding_dim)\n",
        "\n",
        "    def call(self, x):\n",
        "        x = self.fc(x)\n",
        "        x = tf.nn.relu(x)\n",
        "        return x"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hx_W_TiKDCNU"
      },
      "source": [
        "class RNN_Decoder(tf.keras.Model):\n",
        "  def __init__(self, embedding_dim, units, vocab_size):\n",
        "    super(RNN_Decoder, self).__init__()\n",
        "    self.units = units\n",
        "\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(self.units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "    self.fc1 = tf.keras.layers.Dense(self.units)\n",
        "    self.fc2 = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "    self.attention = BahdanauAttention(self.units)\n",
        "\n",
        "  def call(self, x, features, hidden):\n",
        "    # defining attention as a separate model\n",
        "    context_vector, attention_weights = self.attention(features, hidden)\n",
        "\n",
        "    # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
        "    x = self.embedding(x)\n",
        "\n",
        "    # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
        "    x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
        "\n",
        "    # passing the concatenated vector to the GRU\n",
        "    output, state = self.gru(x)\n",
        "\n",
        "    # shape == (batch_size, max_length, hidden_size)\n",
        "    x = self.fc1(output)\n",
        "\n",
        "    # x shape == (batch_size * max_length, hidden_size)\n",
        "    x = tf.reshape(x, (-1, x.shape[2]))\n",
        "\n",
        "    # output shape == (batch_size * max_length, vocab)\n",
        "    x = self.fc2(x)\n",
        "\n",
        "    return x, state, attention_weights\n",
        "\n",
        "  def reset_state(self, batch_size):\n",
        "    return tf.zeros((batch_size, self.units))"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kITtnyUYDFnk"
      },
      "source": [
        "encoder = CNN_Encoder(embedding_dim)\n",
        "decoder = RNN_Decoder(embedding_dim, units, vocab_size)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "etpFa5X_Gycn"
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam()\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')\n",
        "\n",
        "\n",
        "def loss_function(real, pred):\n",
        "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "  loss_ = loss_object(real, pred)\n",
        "\n",
        "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "  loss_ *= mask\n",
        "\n",
        "  return tf.reduce_mean(loss_)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FRFTXs-yG3dO"
      },
      "source": [
        "checkpoint_path = \"/content/drive/MyDrive/Kaggle/checkpoints\"\n",
        "ckpt = tf.train.Checkpoint(encoder=encoder,\n",
        "                           decoder=decoder,\n",
        "                           optimizer=optimizer)\n",
        "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=1)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h2iLlNVIG8bG"
      },
      "source": [
        "start_epoch = 0\n",
        "if ckpt_manager.latest_checkpoint:\n",
        "  start_epoch = int(ckpt_manager.latest_checkpoint.split('-')[-1])\n",
        "  # restoring the latest checkpoint in checkpoint_path\n",
        "  ckpt.restore(ckpt_manager.latest_checkpoint)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bJGyk1QWHBKT"
      },
      "source": [
        "loss_plot = []\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XmHW_WY3HHK_"
      },
      "source": [
        "@tf.function\n",
        "def train_step(img_tensor, target):\n",
        "  loss = 0\n",
        "\n",
        "  # initializing the hidden state for each batch\n",
        "  # because the captions are not related from image to image\n",
        "  hidden = decoder.reset_state(batch_size=target.shape[0])\n",
        "\n",
        "  dec_input = tf.expand_dims([tok_2_int['<START>']] * target.shape[0], 1)\n",
        "\n",
        "  with tf.GradientTape() as tape:\n",
        "      features = encoder(img_tensor)\n",
        "\n",
        "      for i in range(1, target.shape[1]):\n",
        "          # passing the features through the decoder\n",
        "          predictions, hidden, _ = decoder(dec_input, features, hidden)\n",
        "\n",
        "          loss += loss_function(target[:, i], predictions)\n",
        "\n",
        "          # using teacher forcing\n",
        "          dec_input = tf.expand_dims(target[:, i], 1)\n",
        "\n",
        "  total_loss = (loss / int(target.shape[1]))\n",
        "\n",
        "  trainable_variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "\n",
        "  gradients = tape.gradient(loss, trainable_variables)\n",
        "\n",
        "  optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
        "\n",
        "  return loss, total_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LCnfjpZcHIUn"
      },
      "source": [
        "EPOCHS = 50\n",
        "\n",
        "for epoch in range(start_epoch, EPOCHS):\n",
        "    start = time.time()\n",
        "    t = time.time()\n",
        "    total_loss = 0\n",
        "\n",
        "    for (batch, (img_tensor, target)) in enumerate(dataset):\n",
        "        batch_loss, t_loss = train_step(img_tensor, target)\n",
        "        total_loss += t_loss\n",
        "\n",
        "        if batch % 100 == 0:\n",
        "            average_batch_loss = batch_loss.numpy()/int(target.shape[1])\n",
        "            print(f'Epoch {epoch+1} Batch {batch} Loss {average_batch_loss:.4f} time {time.time() - t}')\n",
        "            t = time.time()\n",
        "    # storing the epoch end loss value to plot later\n",
        "    loss_plot.append(total_loss / num_steps)\n",
        "    if epoch % 1 == 0:\n",
        "      ckpt_manager.save()\n",
        "\n",
        "    print(f'Epoch {epoch+1} Loss {total_loss/num_steps:.6f}')\n",
        "    print(f'Time taken for 1 epoch {time.time()-start:.2f} sec\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dAM25SbNylpf"
      },
      "source": [
        "def evaluate(image):\n",
        "   \n",
        "    hidden = decoder.reset_state(batch_size=1)\n",
        "\n",
        "    img = tf.io.read_file(image)\n",
        "    img = tf.image.decode_jpeg(img, channels=3)\n",
        "    img = tf.image.resize(img, (299, 299))\n",
        "    img = tf.keras.applications.inception_v3.preprocess_input(img)\n",
        "    batch_features = image_features_extract_model(tf.expand_dims(img,0))\n",
        "    img_tensor_val = tf.reshape(batch_features, (batch_features.shape[0],\n",
        "                                                 -1,\n",
        "                                                 batch_features.shape[3]))\n",
        "\n",
        "    features = encoder(img_tensor_val)\n",
        "\n",
        "    dec_input = tf.expand_dims([tok_2_int['<START>']], 0)\n",
        "    result = []\n",
        "\n",
        "    for i in range(300):\n",
        "        predictions, hidden, attention_weights = decoder(dec_input,\n",
        "                                                         features,\n",
        "                                                         hidden)\n",
        "\n",
        "        predicted_id = tf.random.categorical(predictions, 1)[0][0].numpy()\n",
        "        result.append(int_2_tok[predicted_id])\n",
        "\n",
        "        if int_2_tok[predicted_id] == '<END>':\n",
        "            return result\n",
        "\n",
        "        dec_input = tf.expand_dims([predicted_id], 0)\n",
        "        str = ''\n",
        "        for s in result[:-1]:\n",
        "          str+=s\n",
        "  \n",
        "    return result"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KHxp7uhKzDKK"
      },
      "source": [
        "test_images = glob.glob('kaggle/test/*/*/*/*.png')"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MYjWM_GB4ouH"
      },
      "source": [
        "image_id = [t.split('/')[-1].split('.')[0] for t in test_images]"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "94sIqH9s2v8y"
      },
      "source": [
        "result = []"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e7PYElh-3LR2"
      },
      "source": [
        "for image in test_images[:10000]:\n",
        "  r = evaluate(image)\n",
        "  res = ''\n",
        "  for s in r[:-1]:\n",
        "    res+=s\n",
        "  result.append(res)\n",
        "  if len(result)%100 == 0:\n",
        "    print(len(result))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PUe7Ydxq_dx7"
      },
      "source": [
        "df = pd.DataFrame({'image_id': image_id, 'InChl':result})\n",
        "df.to_csv('submission.csv')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}